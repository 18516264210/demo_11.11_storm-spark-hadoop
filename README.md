-_11.11_storm-spark-hadoop
==========================

hadoop_storm_spark结合实验的例子，模拟淘宝双11节，根据订单详细信息，汇总出总销售量，各个省份销售排行，以及后期根的SQL分析。--------大概流程-------(1)用户订单入kafka队列，经过storm，实时计算出总销售量，和各个省份的的销售量，将计算结果通过定时任务保存到mysql数据库中。在storm中将kafka中的原始的详单录入hbase，为后期详单查询所用。后期使用spark对hbase中的详单进行批量分析，将分析结果录入redis。使用mahout对详单做机器学习。编码中。2014-11-06
